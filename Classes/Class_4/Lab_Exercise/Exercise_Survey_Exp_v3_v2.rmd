---
title: "Exercise: Analyzing Survey Experiments"
output: pdf_document
fontsize: 16pt
---

```{r setup, include=FALSE, warning=F, message=F}
knitr::opts_chunk$set(echo = T, include=F)
library(tidyverse)
library(broom)
library(estimatr)
library(stargazer)
library(texreg)
library(list)
library(zeligverse)
library(knitr)
```

\textbf{Download the datasets from the website containing responses to survey experiments. Answer the following questions:}

```{r, warning=F, message=F}
survey <- read_csv("Survey_data.csv")
```

### Priming Experiment

`Survey_data.csv`

1. Respondents were primed with one of two questions (variable `Primed`, prime 'hope'=0 and prime 'anger'=1). How does this prime affect subsequent answers to the question about the need for constitutional reform (variable `Constitution_Reform_Support`)? Calculate the difference-in-means estimate of the average treatment effect. Interpret the result.

```{r}
survey %>% t.test(Constitution_Reform_Support~Primed, data=.)
```

2. Perform the same analysis as in Q1, but using a simple OLS regression. 

```{r, results='asis'}
survey %>% lm(Constitution_Reform_Support ~ Primed, data=.) %>% stargazer(digits=3, header=F)
```


### List Experiment

`Survey_data.csv`

The data is for the list experiment of how many activities respondents have done in the past one year, with three items in control and a fourth item relating to clientelism in the treatment.

The number of items the respondent says they have done are contained in the variable `list_response`. If they were shown a control list (3 items) the variable `list_treated` is equal to zero, and if they were shown a treatment list (4 items) the variable `list_treated` is equal to one.

3. Calculate the average treatment effect by subtracting the mean number of responses between the treated and control lists. Interpret the results in terms of the proportion of respondents who have experienced the sensitive item.

```{r}
survey %>% t.test(list_response~list_treated, data=.)
#OR
survey %>% group_by(list_treated) %>%
  summarize(mean=mean(list_response,na.rm=T)) %>%
  mutate(ATE=mean-lag(mean)) %>%
  kable(digits=4)
```

4. Conduct the same analysis using a simple OLS regression. What is the 95\% confidence interval of the proportion of respondents who received a clientelist offer?

```{r}
survey %>% lm(list_response~list_treated, data=.) %>%
  tidy() %>%
  mutate(conf.lo=estimate-std.error*1.96,
         conf.hi=estimate+std.error*1.96) %>%
  filter(term=="list_treated") %>%
  select(conf.lo, conf.hi) %>%
  kable(digits=4)
  
```

5. Now let's check the assumptions of the list experiment. First, check if there is a design effect using the function `ict.test` in the `list` package (for R only). (Note you will need to remove missing values of the `list_response` variable before running the test). Interpret the results based on the explanation in the outcome of `Ã­ct.test`.

```{r}
survey_no_na <- survey %>% filter(is.na(list_response)==F)

ict.test(survey_no_na$list_response, survey_no_na$list_treated, J=3)
```

6. Next, let's check for floor and ceiling effects. There is a complex statistical test for this in the `list` package using the `ictreg` function (in R only): Try the code below and interpret the 'floor' and 'ceiling' parameters to see if they are statistically significant from zero. These are the estimates of whether anyone who should have answered '4' actually lied and answered '3', or who should have answered '1' actually lied and answered '0'. 

```{r, eval=T, echo=T}
ictreg(list_response~1,
       data=survey %>% as.data.frame(),
       treat="list_treated",
       J=3, 
       method="ml",
       floor=T,
       ceiling=T,
       floor.fit="bayesglm",
       ceiling.fit="bayesglm") %>% 
  summary()
```

7. The survey also asked people directly whether they had experienced the sensitive clientelism item (Has anyone ever offered you a gift, some food or money in exchange for your vote?) in variable `direct_clientelism`. Compare the non-response rate (NA responses) to the direct and indirect questions. Does this justify the use of a list experiment or not?

```{r}
survey %>% summarize(na_list=(sum(is.na(list_response))/n())*100,
                     na_direct=(sum(is.na(direct_clientelism))/n())*100) %>%
  kable(digits=4)
```

8. Compare the estimate of the incidence of clientelism from the direct responses to the indirect estimate from the list experiment. What does this suggest about the level of social desirability bias?

```{r}
survey %>% summarize(direct_mean=mean(direct_clientelism,na.rm=T)) %>%
  kable(digits=4)
```

9. Are men or women more likely to have experienced the sensitive item (clientelism)? Run an OLS regression with an interaction between the list treatment and gender to find out. Interpret the results. 

```{r}
survey %>% lm(list_response~list_treated*gender, data=.) %>% 
  tidy() %>%
  kable(digits=4)

```

### Conjoint Experiment

`Conjoint_data.csv`

Respondents to a household survey were shown pairs of candidate profiles with different characteristics and asked which candidate they would vote for. 

The dataset is arranged with one row for every candidate that each respondent assessed (2 experiments \* 2 candidates \* 4047 respondents). So there are four rows for every respondent - two experiments with two candidates in each. Column `UID` identifies each respondent, column `Round` describes whether it was the first or second experiment, and `Choice` identifies each candidate presented in each experiment. 

The first columns (starting `Profile_...`) describe the attributes of each candidate profile. The variable `conjoint_choice` is a binary indicator of which of the two candidate profiles the respondent choose to vote for. There are also columns for the characteristics of the respondent (gender, age, and whether they are a co-ethnic of the candidate profile).

```{r, warning=F, message=F}
conjoint <- read_csv("Conjoint_data.csv")
```

10. How many possible combinations of attributes for a single profile are there? Identify how many unique values are possible for each of the five attributes and multiply these together.

```{r}
2*3*5*2*2
```

11. Run a simple OLS regression to evaluate how the 'Public Goods (PG)' attribute of a Profile affects the respondent's choice of candidate.

```{r, results='asis'}
conjoint %>% zelig(conjoint_choice~1+Profile_PG, data=.,model="ls", cite=F) %>%
  from_zelig_model() %>%
  stargazer(header=F)

conjoint %>% lm(conjoint_choice~Profile_PG, data=.) %>%
  tidy() %>%
  kable(digits=4)

```

12. Since our outcome is a binary variable, run the same regression but with a logit model.

```{r, results='asis'}
conjoint %>% zelig(conjoint_choice~1+Profile_PG, data=.,model="logit")%>%
  from_zelig_model() %>%
  stargazer(header=F)
```

13. Since each respondent participated in two experiments, their answers are likely to be highly correlated. So we have less 'N' than we think. Cluster the standard errors of your OLS regression according to the respondent identifier (UID).

```{r, results='asis'}
conjoint %>% lm_robust(conjoint_choice~1+Profile_PG, data=., clusters=UID)%>%
  texreg()
```

14. Assess the influence of all of the five profile attributes at the same time in an OLS regression. Interpret the results.

```{r, results='asis', tidy=T}
conjoint %>% zelig(conjoint_choice~1+Profile_Gender + Profile_Caste + Profile_Party + Profile_PG + Profile_Promise, data=.,model="ls")%>%
  from_zelig_model() %>%
  stargazer(header=F)
```

15. We can also assess how respondents' characteristics affect their choice. Does the importance of the 'Promise' attribute vary by gender? Use an interaction term between the Promise attribute and respondent gender (ignoring all other variables), and interpret the results. 

```{r, results='asis'}
conjoint %>% zelig(conjoint_choice~1+Profile_Promise*respondent_gender, data=.,model="logit")%>%
  from_zelig_model() %>%
  stargazer(header=F)
```
