---
title: "Exercise: Controlling for Confounding"
output: pdf_document
fontsize: 16pt
---

```{r setup, echo=F,warning=F, error=F, message=F}
knitr::opts_chunk$set(echo = F, warning=F, error=F, message=F, fig.height=3, fig.width=3, include=F)
library(tidyverse)
library(stargazer)
library(estimatr)
```

Let's simulate some fake data and see when we are able to recover the correct treatment effect just by controlling for covariates. 

1. First, let's generate two potential confounder variables for 10,000 people. 
(a) The variable 'income' should be drawn randomly from the normal distribution with mean 500 and standard deviation 50.
(b) The variable 'gender' should be drawn randomly from the binomial distribution with a 0.5 probability of being male or female.

```{r}
set.seed(54321)
N <- 10000
d <- tibble(income=rnorm(N,500,50),
            gender=rbinom(N,1,0.5))
```

2. Our outcome is going to be attitudes to redistribution. Use the expressions below to simulate potential outcomes, with a treatment effect of 5.

$$y_0=N(10,2)+\frac{income}{100} - 2*gender$$ 

$$y_1=y_0+5$$

```{r}
d <- d %>% mutate(y_0=rnorm(N,10,2) + income/10 - 12*gender,
                  y_1=y_0+5)
```

3. Treatment $D$ is receiving a government social program, but treatment is **not** randomly  assigned. Instead, treatment depends on income and gender. Binary (1/0) treatment is determined by the following expression:

$$D=\cases{1 \text{ if } 100*gender + income>450 \\ 0 \text{ else }}$$

```{r}
d <- d %>% mutate(D=case_when(50*gender+income>525~1,
                              T~0))
```

4. Calculate observed outcomes based on potential outcomes and treatment.

```{r}
d <- d %>% mutate(y_obs=case_when(D==0~y_0,
                                  D==1~y_1))

```

5. Let's start with the 'naive' regression of the outcome on treatment with no controls. How does the result compare to our assumed treatment effect? 

```{r}
d %>% lm(y_obs~D, data=.) %>% summary()
```

6. Is the result of the 'naive' regression larger or smaller than it should be? Why? Be specific and be concrete.

7. Draw (by hand) the causal diagram (DAG) associated with the variables and equations we have generated above. 

8. Identify the back-door paths connecting treatment ($D$) and the outcome ($Y$). To **block** these back-door paths, which variables do we need to control for?

9. Run a regression that controls for the variables you identified in Q8. Compare the result to the 'correct' treatment effect we assumed at the start.

```{r}
d %>% lm(y_obs~D + gender + income, data=.) %>% summary()
```

10. That was the easy part. Now let's try a more complicated example.





10. What happens if we run the regression which adds ZZZ as a control variable, even though it is 'post-treatment'?

```{r}
d %>% lm(y_obs~D + gender + income, data=.) %>% summary()
```


4. Treatment $D$ is participation by the municipality in a federal government program and occurs between time periods $t=1$ and $t=2$. We will assume that only municipalities with oil receive treatment. Make an indicator variable where each municipality with oil is coded as being in the treated group and the rest as control. (*Note* that we are not coding municipalities as treated only in $t=2$, we are coding for whether they are a treated 'unit' which applies even in $t=0$, $t=1$).

```{r}
d <- d %>% mutate(D=oil)
```

5. Now calculate the observed outcome based on the potential outcomes, treatment status *AND time period*.

```{r}
d <- d %>% mutate(y_obs=case_when(D==0~y_0,
                                  D==1 & year<2~y_0,
                                  D==1 & year==2~y_1))
```

6. First, let's run the 'naive' \textbf{cross-sectional} observational regression of observed outcomes on treatment. How does the result compare to our simulation assumptions? Why?

```{r}
d %>% lm(y_obs ~D, data=.) %>% summary()
```

7. Next, let's run the 'naive' \textbf{before-after} regression of observed outcomes on year, using the data from $t=1$ and $t=2$ only for the treated units.  How does the result compare to our simulation assumptions? Why?

```{r}
d %>% filter(year>0 & D==1) %>% 
  lm(y_obs ~ year, data=.) %>% summary()
```

8. Now, using the data for $t=1$ and $t=2$, let's run a basic difference-in-differences regression of the observed outcomes on treatment, year, and the interaction of time and year. How do you interpret the results?

```{r}
d %>% filter(year>0) %>% 
  lm(y_obs ~ D + year + D*year, data=.) %>% summary()
```

9. Our standard errors are wrong here. Cluster your errors by the cross-sectional unit (municipality). (In this case the difference is very small).

```{r}
d %>% filter(year>0) %>% 
  lm_robust(y_obs ~ D + year + D*year, data=., cluster=municipality) %>% summary()
```

10. We can also do a simpler differences-in-differences-in-means estimate without a regression. Create a 2*2 table of average outcomes for the four groups as shown in the table below. Then calculate the differences in the rows and/or the columns, and finally the difference in the differences. Interpret your result.

Treatment:    | D=0 | D=1
----|-----|------
t=1 |     | 
t=2 |     | 

```{r}
d_in_d_means <- d %>% filter(year>0) %>% 
  group_by(D, year) %>% 
  summarize(mean_y=mean(y_obs,na.rm=T))

d_in_d_means %>%
  spread(key="year", value="mean_y") %>%
  ungroup() %>%
  mutate(Diff=`2`-`1`,
         Diff_Diff=Diff-lag(Diff))
```

11. An assumption of Difference-in-Differences is that there are parallel trends before treatment occurs. Test whether the treated and countrol groups display parallel trends in the outcome variable between time $t=0$ and $t=1$. One way to do this is to run exactly the same difference-in-differences regression but excluding time $t=2$. Interpret your results.

```{r}
d %>% filter(year<2) %>% 
  lm(y_obs ~ D + year + D*year, data=.) %>% summary()
```

12. Plot a classic difference-in-differences line graph of the average observed outcome, where the x-axis contains the three time periods, the y-axis the average outcome, and there is one line for the treatment group and one for the control group.

```{r}
d %>% group_by(D, year) %>% 
  summarize(mean_y=mean(y_obs,na.rm=T)) %>% 
  ggplot() +
  geom_line(aes(x=year, y=mean_y, group=factor(D), colour=factor(D))) +
  theme_classic()
```


13. Finally, let's try to see what estimate we recover when there are **non-parallel trends** produced by **time-varying confounders**. Recreate your dataset but with the following structure of potential outcomes, which only differs in that the falling turnout trend is only present in oil municipalities. (Remember to calculate observed outcomes again). Interpret the results of the difference-in-differences regression this time.

$$y_{0, year}=N(60,5) - 2*year*oil - 3*oil$$ 

$$y_{1, year}=y_{0, year}+5$$


```{r}
d2 <- d %>% mutate(y_0=rnorm(N,60,5) - 2*year*oil - 3*oil,
                  y_1=y_0+5)  %>% 
  mutate(y_obs=case_when(D==0~y_0,
                         D==1 & year<2~y_0,
                         D==1 & year==2~y_1))

d2 %>% filter(year>0) %>% 
  lm(y_obs ~ D + year + D*year, data=.) %>% summary()
```

14. Create the difference-in-differences line graph for this new dataset with time-varying confounders. (The same as in Q12).

```{r}
d2 %>% group_by(D, year) %>% 
  summarize(mean_y=mean(y_obs,na.rm=T)) %>% 
  ggplot() +
  geom_line(aes(x=year, y=mean_y, group=factor(D), colour=factor(D))) +
  theme_classic()
```